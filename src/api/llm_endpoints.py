"""
API endpoints for LLM service integration.

This module provides REST endpoints for LLM-based validation,
health checks, and recommendation generation.
"""

from typing import Dict, List, Any, Optional
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, Field
import logging

from ..services.llm_service import llm_service, ValidationRequest
from ..utils.exceptions import LLMServiceError, LLMValidationError
from .dependencies import get_current_user


logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/llm", tags=["LLM Service"])


class LLMHealthResponse(BaseModel):
    """Response model for LLM health check."""
    status: str = Field(..., description="Service status")
    enabled: bool = Field(..., description="Whether LLM service is enabled")
    model: Optional[str] = Field(None, description="LLM model being used")
    response_time_ms: Optional[float] = Field(None, description="Response time in milliseconds")
    cache_size: Optional[int] = Field(None, description="Number of cached responses")
    error: Optional[str] = Field(None, description="Error message if unhealthy")


class ValidationRequestModel(BaseModel):
    """Request model for LLM validation."""
    validation_type: str = Field(..., description="Type of validation to perform")
    model_metrics: Dict[str, Any] = Field(..., description="Model performance metrics")
    optimization_config: Dict[str, Any] = Field(..., description="Optimization configuration")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context")


class ValidationResponseModel(BaseModel):
    """Response model for LLM validation."""
    is_valid: bool = Field(..., description="Whether the optimization is valid")
    confidence_score: float = Field(..., description="Confidence score (0.0 to 1.0)")
    reasoning: str = Field(..., description="LLM reasoning for the assessment")
    recommendations: List[str] = Field(..., description="List of recommendations")
    warnings: List[str] = Field(..., description="List of warnings")
    errors: List[str] = Field(..., description="List of errors")
    metadata: Dict[str, Any] = Field(..., description="Additional metadata")


class RecommendationRequestModel(BaseModel):
    """Request model for generating recommendations."""
    model_metrics: Dict[str, Any] = Field(..., description="Current model metrics")
    optimization_config: Dict[str, Any] = Field(..., description="Optimization configuration used")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context")


class RecommendationResponseModel(BaseModel):
    """Response model for recommendations."""
    recommendations: List[str] = Field(..., description="List of recommendations")
    generated_by_llm: bool = Field(..., description="Whether recommendations were generated by LLM")
    model_used: Optional[str] = Field(None, description="LLM model used for generation")
    confidence_score: Optional[float] = Field(None, description="Confidence in recommendations")


@router.get("/health", response_model=LLMHealthResponse)
async def get_llm_health():
    """
    Get LLM service health status.
    
    Returns health information including availability, response time,
    and cache statistics.
    """
    try:
        health_data = await llm_service.health_check()
        
        return LLMHealthResponse(
            status=health_data.get("status", "unknown"),
            enabled=llm_service.enabled,
            model=health_data.get("model"),
            response_time_ms=health_data.get("response_time_ms"),
            cache_size=health_data.get("cache_size"),
            error=health_data.get("error")
        )
        
    except Exception as e:
        logger.error(f"LLM health check failed: {e}")
        return LLMHealthResponse(
            status="error",
            enabled=llm_service.enabled,
            error=str(e)
        )


@router.post("/validate", response_model=ValidationResponseModel)
async def validate_optimization_result(
    request: ValidationRequestModel,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Validate optimization results using LLM analysis.
    
    Performs intelligent validation of model optimization results,
    providing reasoning, recommendations, and confidence scores.
    """
    if not llm_service.is_available():
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="LLM service is not available"
        )
    
    try:
        # Create validation request
        validation_request = ValidationRequest(
            validation_type=request.validation_type,
            model_metrics=request.model_metrics,
            optimization_config=request.optimization_config,
            context=request.context
        )
        
        # Perform validation
        result = await llm_service.validate_optimization_result(validation_request)
        
        logger.info(
            f"LLM validation completed for user {current_user.get('user_id')}",
            extra={
                "component": "LLMEndpoints",
                "user_id": current_user.get("user_id"),
                "validation_type": request.validation_type,
                "is_valid": result.is_valid,
                "confidence": result.confidence_score
            }
        )
        
        return ValidationResponseModel(
            is_valid=result.is_valid,
            confidence_score=result.confidence_score,
            reasoning=result.reasoning,
            recommendations=result.recommendations,
            warnings=result.warnings,
            errors=result.errors,
            metadata=result.metadata
        )
        
    except LLMValidationError as e:
        logger.error(f"LLM validation error: {e}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"Validation failed: {str(e)}"
        )
    except LLMServiceError as e:
        logger.error(f"LLM service error: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"LLM service error: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Unexpected error during LLM validation: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during validation"
        )


@router.post("/recommendations", response_model=RecommendationResponseModel)
async def generate_recommendations(
    request: RecommendationRequestModel,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Generate optimization recommendations using LLM.
    
    Analyzes model metrics and optimization configuration to provide
    intelligent recommendations for improvement.
    """
    try:
        if llm_service.is_available():
            # Generate LLM recommendations
            recommendations = await llm_service.generate_recommendations(
                model_metrics=request.model_metrics,
                optimization_config=request.optimization_config,
                context=request.context
            )
            
            logger.info(
                f"Generated {len(recommendations)} LLM recommendations for user {current_user.get('user_id')}",
                extra={
                    "component": "LLMEndpoints",
                    "user_id": current_user.get("user_id"),
                    "recommendation_count": len(recommendations)
                }
            )
            
            return RecommendationResponseModel(
                recommendations=recommendations,
                generated_by_llm=True,
                model_used=llm_service.model,
                confidence_score=0.8  # Default confidence for recommendations
            )
        else:
            # Fallback to basic recommendations
            fallback_recommendations = [
                "LLM service unavailable - using fallback recommendations",
                "Consider reviewing optimization parameters",
                "Check model performance against target metrics",
                "Validate results on representative test data"
            ]
            
            return RecommendationResponseModel(
                recommendations=fallback_recommendations,
                generated_by_llm=False,
                model_used=None,
                confidence_score=None
            )
            
    except LLMServiceError as e:
        logger.error(f"LLM service error: {e}")
        # Return fallback recommendations instead of error
        return RecommendationResponseModel(
            recommendations=[f"LLM service error: {str(e)}"],
            generated_by_llm=False,
            model_used=None,
            confidence_score=None
        )
    except Exception as e:
        logger.error(f"Unexpected error generating recommendations: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error generating recommendations"
        )


@router.post("/cache/clear")
async def clear_llm_cache(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Clear LLM response cache.
    
    Clears all cached LLM responses. Useful for testing or when
    you want fresh responses from the LLM service.
    """
    try:
        llm_service.clear_cache()
        
        logger.info(
            f"LLM cache cleared by user {current_user.get('user_id')}",
            extra={
                "component": "LLMEndpoints",
                "user_id": current_user.get("user_id")
            }
        )
        
        return {"message": "LLM cache cleared successfully"}
        
    except Exception as e:
        logger.error(f"Failed to clear LLM cache: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to clear cache"
        )


@router.get("/config")
async def get_llm_config(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Get LLM service configuration.
    
    Returns current LLM service configuration (without sensitive data).
    """
    try:
        config = {
            "enabled": llm_service.enabled,
            "model": llm_service.model,
            "base_url": llm_service.base_url,
            "timeout_seconds": llm_service.timeout_seconds,
            "max_retries": llm_service.max_retries,
            "cache_enabled": llm_service.cache_enabled,
            "cache_ttl_seconds": llm_service.cache_ttl_seconds,
            "validation_enabled": llm_service.validation_enabled,
            "confidence_threshold": llm_service.confidence_threshold,
            "max_tokens": llm_service.max_tokens,
            "is_available": llm_service.is_available()
        }
        
        return config
        
    except Exception as e:
        logger.error(f"Failed to get LLM config: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get configuration"
        )